#!/usr/bin/env python3
"""
é«˜é¢‘æ•°æ®åŠ¨ä½œåºåˆ—é¢„å¤„ç†æ¨¡å—
å®ç°åŠ¨æ€ä¸‹é‡‡æ ·å’Œchunkingç­–ç•¥
"""

import numpy as np
import torch
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
import json
from collections import defaultdict
import matplotlib.pyplot as plt


class ActionChunkPreprocessor:
    """åŠ¨ä½œå—é¢„å¤„ç†å™¨"""
    
    def __init__(self, 
                 chunk_length: int = 16,
                 min_action_variance: float = 0.001,
                 max_downsample_factor: int = 8,
                 overlap_ratio: float = 0.5,  # æ–°å¢é‡å æ¯”ä¾‹
                 normalize_data: bool = True):  # æ–°å¢æ•°æ®æ ‡å‡†åŒ–
        """
        åˆå§‹åŒ–é¢„å¤„ç†å™¨
        
        Args:
            chunk_length: åŠ¨ä½œå—é•¿åº¦
            min_action_variance: æœ€å°åŠ¨ä½œæ–¹å·®ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦ä¸‹é‡‡æ ·
            max_downsample_factor: æœ€å¤§ä¸‹é‡‡æ ·å› å­
            overlap_ratio: é‡å æ¯”ä¾‹
            normalize_data: æ˜¯å¦æ ‡å‡†åŒ–æ•°æ®
        """
        self.chunk_length = chunk_length
        self.min_action_variance = min_action_variance
        self.max_downsample_factor = max_downsample_factor
        self.overlap_ratio = overlap_ratio
        self.normalize_data = normalize_data
        
        # æ•°æ®æ ‡å‡†åŒ–å‚æ•°
        self.data_min = -2.9459
        self.data_max = 1.4640
        self.data_mean = (self.data_min + self.data_max) / 2.0
        self.data_std = (self.data_max - self.data_min) / 4.0  # è¦†ç›–99.7%çš„æ•°æ®
    
    def normalize_actions(self, actions: np.ndarray) -> np.ndarray:
        """æ ‡å‡†åŒ–åŠ¨ä½œæ•°æ®åˆ°[-1, 1]èŒƒå›´"""
        if not self.normalize_data:
            return actions
        
        # æ ‡å‡†åŒ–åˆ°[-1, 1]
        normalized = (actions - self.data_mean) / self.data_std
        # ç¡®ä¿åœ¨[-1, 1]èŒƒå›´å†…
        normalized = np.clip(normalized, -1.0, 1.0)
        return normalized
    
    def denormalize_actions(self, normalized_actions: np.ndarray) -> np.ndarray:
        """åæ ‡å‡†åŒ–åŠ¨ä½œæ•°æ®"""
        if not self.normalize_data:
            return normalized_actions
        
        # ä»[-1, 1]åæ ‡å‡†åŒ–
        actions = normalized_actions * self.data_std + self.data_mean
        return actions
    
    def analyze_task_data(self, trajectory_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†æä»»åŠ¡æ•°æ®ç‰¹å¾
        
        Args:
            trajectory_data: è½¨è¿¹æ•°æ®
            
        Returns:
            æ•°æ®åˆ†æç»“æœ
        """
        actions = np.array(trajectory_data.get('actions', []))
        
        if len(actions) == 0:
            return {'error': 'No actions found'}
        
        # è®¡ç®—åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        total_frames = len(actions)
        action_dim = actions.shape[1] if len(actions.shape) > 1 else 1
        
        # è®¡ç®—åŠ¨ä½œå˜åŒ–ç»Ÿè®¡
        if total_frames > 1:
            action_diffs = np.diff(actions, axis=0)
            action_variances = np.var(action_diffs, axis=0)
            mean_variance = np.mean(action_variances)
            max_variance = np.max(action_variances)
            min_variance = np.min(action_variances)
        else:
            mean_variance = 0.0
            max_variance = 0.0
            min_variance = 0.0
        
        # è®¡ç®—æ¯ä¸ªå…³èŠ‚çš„å¹³å‡å˜åŒ–é‡
        joint_changes = np.mean(np.abs(action_diffs), axis=0) if total_frames > 1 else np.zeros(action_dim)
        
        return {
            'total_frames': total_frames,
            'action_dim': action_dim,
            'mean_action_variance': mean_variance,
            'max_action_variance': max_variance,
            'min_action_variance': min_variance,
            'mean_joint_changes': joint_changes.tolist(),
            'total_action_range': float(actions.max() - actions.min()),
            'action_std': float(np.std(actions)),
            'action_mean': float(np.mean(actions))
        }
    
    def determine_downsample_factor(self, analysis: Dict[str, Any]) -> int:
        """
        ç¡®å®šä¸‹é‡‡æ ·å› å­
        
        Args:
            analysis: æ•°æ®åˆ†æç»“æœ
            
        Returns:
            ä¸‹é‡‡æ ·å› å­
        """
        mean_variance = analysis['mean_action_variance']
        
        # å¦‚æœåŠ¨ä½œå˜åŒ–å¤ªå°ï¼Œéœ€è¦ä¸‹é‡‡æ ·
        if mean_variance < self.min_action_variance:
            # è®¡ç®—éœ€è¦çš„ä¸‹é‡‡æ ·å› å­
            target_variance = self.min_action_variance * 2
            if mean_variance > 0:
                factor = int(np.sqrt(target_variance / mean_variance))
                factor = min(factor, self.max_downsample_factor)
                factor = max(factor, 1)
            else:
                factor = self.max_downsample_factor
        else:
            factor = 1
        
        return factor
    
    def downsample_actions(self, actions: np.ndarray, factor: int) -> np.ndarray:
        """
        ä¸‹é‡‡æ ·åŠ¨ä½œåºåˆ— - ä½¿ç”¨å›ºå®šé—´éš”é‡‡æ ·ä»¥ä¿æŒæ—¶é—´ä¸€è‡´æ€§
        
        Args:
            actions: åŸå§‹åŠ¨ä½œåºåˆ—
            factor: ä¸‹é‡‡æ ·å› å­
            
        Returns:
            ä¸‹é‡‡æ ·åçš„åŠ¨ä½œåºåˆ—
        """
        if factor <= 1:
            return actions
        
        # ä½¿ç”¨å›ºå®šé—´éš”é‡‡æ ·ï¼Œä¿æŒæ—¶é—´ä¸€è‡´æ€§
        n_frames = len(actions)
        n_downsampled = n_frames // factor
        
        if n_downsampled == 0:
            return actions[:1]  # è‡³å°‘ä¿ç•™ä¸€å¸§
        
        # å›ºå®šé—´éš”é‡‡æ ·ï¼Œè€Œä¸æ˜¯æ»‘åŠ¨çª—å£å¹³å‡
        indices = np.arange(0, n_frames, factor)[:n_downsampled]
        downsampled = actions[indices]
        
        return downsampled
    
    def create_action_chunks(self, actions: np.ndarray) -> List[np.ndarray]:
        """
        åˆ›å»ºåŠ¨ä½œå—
        
        Args:
            actions: åŠ¨ä½œåºåˆ—
            
        Returns:
            åŠ¨ä½œå—åˆ—è¡¨
        """
        n_frames = len(actions)
        chunks = []
        
        # åœ¨åŠ¨ä½œå—åˆ›å»ºéƒ¨åˆ†ä¿®æ”¹æ­¥é•¿ï¼š
        step_size = max(1, int(self.chunk_length * (1 - self.overlap_ratio)))  # 50%é‡å 
        
        for start_idx in range(0, n_frames - self.chunk_length + 1, step_size):
            end_idx = start_idx + self.chunk_length
            chunk = actions[start_idx:end_idx]
            chunks.append(chunk)
        
        # å¤„ç†å‰©ä½™çš„å¸§
        remaining_frames = n_frames - (len(chunks) * step_size)
        if remaining_frames > 0:
            # ä½¿ç”¨å¡«å……åˆ›å»ºæœ€åä¸€ä¸ªå—
            last_chunk = actions[-remaining_frames:]
            if len(last_chunk) < self.chunk_length:
                # ç”¨æœ€åä¸€å¸§å¡«å……åˆ°å®Œæ•´é•¿åº¦
                padding = np.tile(last_chunk[-1:], (self.chunk_length - len(last_chunk), 1))
                padded_chunk = np.vstack([last_chunk, padding])
                chunks.append(padded_chunk)
            else:
                chunks.append(last_chunk)
        
        return chunks
    
    def process_trajectory(self, trajectory_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªè½¨è¿¹
        
        Args:
            trajectory_data: è½¨è¿¹æ•°æ®
            
        Returns:
            å¤„ç†åçš„è½¨è¿¹æ•°æ®
        """
        print("ğŸ“Š åˆ†æè½¨è¿¹æ•°æ®...")
        analysis = self.analyze_task_data(trajectory_data)
        
        if 'error' in analysis:
            return {'error': analysis['error']}
        
        print(f"   æ€»å¸§æ•°: {analysis['total_frames']}")
        print(f"   åŠ¨ä½œç»´åº¦: {analysis['action_dim']}")
        print(f"   å¹³å‡åŠ¨ä½œæ–¹å·®: {analysis['mean_action_variance']:.6f}")
        print(f"   åŠ¨ä½œæ ‡å‡†å·®: {analysis['action_std']:.6f}")
        
        # ç¡®å®šä¸‹é‡‡æ ·å› å­
        downsample_factor = self.determine_downsample_factor(analysis)
        print(f"   ä¸‹é‡‡æ ·å› å­: {downsample_factor}")
        
        # ä»joint_posè®¡ç®—çœŸå®çš„actionsï¼ˆç›¸é‚»å¸§å·®å€¼ï¼‰
        joint_positions = np.array([obs['joint_pos'] for obs in trajectory_data['observations']])
        # è®¡ç®—actionsï¼šåä¸€å¸§å‡å»å‰ä¸€å¸§
        actions = np.diff(joint_positions, axis=0)
        
        if downsample_factor > 1:
            downsampled_actions = self.downsample_actions(actions, downsample_factor)
            print(f"   ä¸‹é‡‡æ ·åå¸§æ•°: {len(downsampled_actions)}")
        else:
            downsampled_actions = actions
        
        # æ ‡å‡†åŒ–åŠ¨ä½œæ•°æ®
        if self.normalize_data:
            normalized_actions = self.normalize_actions(downsampled_actions)
            print(f"   æ•°æ®æ ‡å‡†åŒ–å®Œæˆï¼ŒèŒƒå›´: [{normalized_actions.min():.4f}, {normalized_actions.max():.4f}]")
        else:
            normalized_actions = downsampled_actions
        
        # åˆ›å»ºåŠ¨ä½œå—
        chunks = self.create_action_chunks(normalized_actions)
        print(f"   åˆ›å»ºåŠ¨ä½œå—æ•°: {len(chunks)}")
        
        # å¤„ç†å…¶ä»–æ•°æ®ï¼ˆobservationsç­‰ï¼‰
        processed_data = {
            'task_name': trajectory_data.get('task_name', 'unknown'),
            'original_analysis': analysis,
            'downsample_factor': downsample_factor,
            'original_frames': analysis['total_frames'],
            'downsampled_frames': len(downsampled_actions),
            'chunk_length': self.chunk_length,
            'num_chunks': len(chunks),
            'action_chunks': [chunk.tolist() for chunk in chunks],
            'chunk_statistics': self._compute_chunk_statistics(chunks),
            'normalized': self.normalize_data,
            'normalization_params': {
                'data_min': self.data_min,
                'data_max': self.data_max,
                'data_mean': self.data_mean,
                'data_std': self.data_std
            }
        }
        
        # å¦‚æœæœ‰observationsï¼Œä¹Ÿè¿›è¡Œç›¸åº”çš„å¤„ç†
        if 'observations' in trajectory_data:
            processed_observations = self._process_observations(
                trajectory_data['observations'], 
                downsample_factor
            )
            processed_data['processed_observations'] = processed_observations
        
        return processed_data
    
    def _compute_chunk_statistics(self, chunks: List[np.ndarray]) -> Dict[str, Any]:
        """
        è®¡ç®—å—ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            chunks: åŠ¨ä½œå—åˆ—è¡¨
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        all_chunks = np.vstack(chunks)
        
        return {
            'mean_action': float(np.mean(all_chunks)),
            'std_action': float(np.std(all_chunks)),
            'min_action': float(np.min(all_chunks)),
            'max_action': float(np.max(all_chunks)),
            'mean_chunk_variance': float(np.mean([np.var(chunk, axis=0).mean() for chunk in chunks])),
            'max_chunk_variance': float(np.max([np.var(chunk, axis=0).max() for chunk in chunks]))
        }
    
    def _process_observations(self, observations: List[Dict[str, Any]], downsample_factor: int) -> List[Dict[str, Any]]:
        """
        å¤„ç†è§‚æµ‹æ•°æ® - ä½¿ç”¨å›ºå®šé—´éš”é‡‡æ ·ä»¥ä¿æŒæ—¶é—´ä¸€è‡´æ€§
        
        Args:
            observations: è§‚æµ‹æ•°æ®åˆ—è¡¨
            downsample_factor: ä¸‹é‡‡æ ·å› å­
            
        Returns:
            å¤„ç†åçš„è§‚æµ‹æ•°æ®
        """
        if downsample_factor <= 1:
            return observations
        
        # ä½¿ç”¨å›ºå®šé—´éš”é‡‡æ ·ï¼Œä¿æŒæ—¶é—´ä¸€è‡´æ€§
        processed = []
        for i in range(0, len(observations), downsample_factor):
            processed.append(observations[i])
        
        return processed
    
    def visualize_preprocessing(self, original_data: Dict[str, Any], processed_data: Dict[str, Any]):
        """
        å¯è§†åŒ–é¢„å¤„ç†ç»“æœ
        
        Args:
            original_data: åŸå§‹æ•°æ®
            processed_data: å¤„ç†åæ•°æ®
        """
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # åŸå§‹åŠ¨ä½œåºåˆ—
        original_actions = np.array(original_data['actions'])
        axes[0, 0].plot(original_actions[:, 0], label='Joint 0')
        axes[0, 0].plot(original_actions[:, 1], label='Joint 1')
        axes[0, 0].plot(original_actions[:, 2], label='Joint 2')
        axes[0, 0].set_title('Original Action Sequence')
        axes[0, 0].set_xlabel('Frame')
        axes[0, 0].set_ylabel('Action Value')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # ä¸‹é‡‡æ ·åçš„åŠ¨ä½œåºåˆ—
        downsampled_actions = np.array(processed_data['action_chunks']).reshape(-1, original_actions.shape[1])
        downsampled_frames = np.arange(len(downsampled_actions)) * processed_data['downsample_factor']
        axes[0, 1].plot(downsampled_frames, downsampled_actions[:, 0], 'o-', label='Joint 0')
        axes[0, 1].plot(downsampled_frames, downsampled_actions[:, 1], 's-', label='Joint 1')
        axes[0, 1].plot(downsampled_frames, downsampled_actions[:, 2], '^-', label='Joint 2')
        axes[0, 1].set_title('Downsampled Action Sequence')
        axes[0, 1].set_xlabel('Frame')
        axes[0, 1].set_ylabel('Action Value')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # åŠ¨ä½œå—å¯è§†åŒ–
        chunk_actions = np.array(processed_data['action_chunks'])
        chunk_idx = 0
        for i, chunk in enumerate(chunk_actions[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ªå—
            chunk_frames = np.arange(len(chunk)) + i * self.chunk_length
            axes[1, 0].plot(chunk_frames, chunk[:, 0], alpha=0.7, label=f'Chunk {i}' if i < 3 else '')
        axes[1, 0].set_title('Action Chunks (First 5 chunks)')
        axes[1, 0].set_xlabel('Frame')
        axes[1, 0].set_ylabel('Action Value')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        analysis = processed_data['original_analysis']
        stats_text = f"""
        Original Frames: {analysis['total_frames']}
        Downsampled Frames: {processed_data['downsampled_frames']}
        Downsample Factor: {processed_data['downsample_factor']}
        Action Chunks: {processed_data['num_chunks']}
        Chunk Length: {processed_data['chunk_length']}
        
        Mean Action Variance: {analysis['mean_action_variance']:.6f}
        Action Std: {analysis['action_std']:.6f}
        """
        axes[1, 1].text(0.1, 0.5, stats_text, transform=axes[1, 1].transAxes, 
                        fontsize=12, verticalalignment='center', fontfamily='monospace')
        axes[1, 1].set_title('Preprocessing Statistics')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.show()


def preprocess_trajectory_file(trajectory_path: str, output_path: str = None, show_visualization: bool = False) -> Dict[str, Any]:
    """
    é¢„å¤„ç†è½¨è¿¹æ–‡ä»¶
    
    Args:
        trajectory_path: è½¨è¿¹æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        show_visualization: æ˜¯å¦æ˜¾ç¤ºå¯è§†åŒ–å›¾è¡¨
        
    Returns:
        å¤„ç†åçš„æ•°æ®
    """
    # åŠ è½½è½¨è¿¹æ•°æ®
    with open(trajectory_path, 'r', encoding='utf-8') as f:
        trajectory_data = json.load(f)
    
    # åˆ›å»ºé¢„å¤„ç†å™¨
    preprocessor = ActionChunkPreprocessor(
        chunk_length=16,
        min_action_variance=0.001,
        max_downsample_factor=8
    )
    
    # å¤„ç†è½¨è¿¹
    processed_data = preprocessor.process_trajectory(trajectory_data)
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(processed_data, f, indent=2, ensure_ascii=False)
        print(f"å¤„ç†åçš„æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    
    # å¯è§†åŒ–ç»“æœï¼ˆä»…åœ¨éœ€è¦æ—¶æ˜¾ç¤ºï¼‰
    if show_visualization:
        preprocessor.visualize_preprocessing(trajectory_data, processed_data)
    
    return processed_data


if __name__ == "__main__":
    # æµ‹è¯•é¢„å¤„ç†æ¨¡å—
    test_trajectory_path = "/root/kuavo_ws/src/vla/trajectories/wave_001.json"
    output_path = "/root/kuavo_ws/src/vla/trajectories/wave_001_processed.json"
    
    processed_data = preprocess_trajectory_file(test_trajectory_path, output_path)
    print("é¢„å¤„ç†å®Œæˆï¼")